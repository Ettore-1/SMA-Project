{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QYwloMQjJ4P"
   },
   "source": [
    "**PERSONS INVOLVED IN THE GROUP:** ### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKcSAOQbtz2H"
   },
   "source": [
    "**Before starting:**\n",
    "\n",
    "You need to install Basemap. You have to do it at each new session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "id": "PHwkL_9OtuBw",
    "outputId": "fc74a3bd-cb79-485a-aea9-ef032fb09bf6"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "apt-get install libgeos-3* libgeos-dev\n",
    "pip install https://github.com/matplotlib/basemap/archive/master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3u5uMF3jJ4Q"
   },
   "source": [
    "**Goal of this practice:**\n",
    "\n",
    "The goal is to use a cloud computing platform (the Google Cloud Platform). We will use big data tools on Colab to process large datasets. These data are real satellite observations of the ocean (5 years)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGcBf2LMjJ4R"
   },
   "source": [
    "**Evaluation of this practice:**\n",
    "\n",
    "I will evaluate 3 main components.\n",
    "\n",
    "The first is the quality of the code. It must be concise and well written. The goal is to manipulate big data tools provided by the Google Cloud Platform (such as BigQuery).\n",
    "\n",
    "The second is the quality of the outputs. Never forget that you are dealing with physical variables evolving in space and time. You must use maps and time series. You must also write the units (e.g., 째C, m).\n",
    "\n",
    "The third thing is the discussion about your results. Again, remember that you are dealing with oceanographic data. They have a physical meaning and you should be able to comment the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPvjGAOAjJ4S"
   },
   "source": [
    "**Import libraries:**\n",
    "\n",
    "- matplotlib (2D plotting)\n",
    "- pylab (scientific computing)\n",
    "- basemap (plotting 2D data on maps)\n",
    "- scikit-learn (machine learning)\n",
    "- pandas (data structures and data analysis tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "7SYwR8rLjJ4T",
    "outputId": "b5e76676-f37b-46d9-a7eb-11643d9244e6"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from scipy.stats.mstats import zscore\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # disable warnings\n",
    "pylab.rcParams['figure.figsize']=(20,20) # graph size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Jw7IngtjJ4W"
   },
   "source": [
    "**Declare functions:**\n",
    "- plot_im (plot satellite image with geographic coordinates)\n",
    "- plot_ts (plot time series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "irVyDROljJ4X"
   },
   "outputs": [],
   "source": [
    "# function to plot images  \n",
    "def plot_im(lon,lat,im,size_points,var_name):\n",
    "    \n",
    "    # transform to arrays (just in case)\n",
    "    lon=array(lon)\n",
    "    lat=array(lat)\n",
    "    im=array(im)\n",
    "    \n",
    "    if max(lon)-min(lon)<100:\n",
    "      # Mercator projection (for small zone)\n",
    "      m=Basemap(projection='merc',llcrnrlat=nanmin(lat),urcrnrlat=nanmax(lat),\\\n",
    "                llcrnrlon=nanmin(lon),urcrnrlon=nanmax(lon),lat_0=(nanmax(lat)+nanmin(lat))*0.5,\\\n",
    "                lon_0=(nanmax(lon)+nanmin(lon))*0.5,resolution='l')\n",
    "    else:\n",
    "      # Orthogonal projection (for large zone)\n",
    "      m=Basemap(projection='ortho',lat_0=0,lon_0=0,resolution='l')\n",
    "    # you can use other projections (see https://matplotlib.org/basemap/users/mapsetup.html)\n",
    "    \n",
    "    # transform (lon,lat) to (x,y)\n",
    "    x,y=m(lon,lat)\n",
    "\n",
    "    # plot\n",
    "    im=ma.masked_where(isnan(im),im)\n",
    "    res=m.scatter(x,y,size_points,im,'o',alpha=1,cmap='jet',lw=0)\n",
    "    m.drawcoastlines()\n",
    "    m.fillcontinents()\n",
    "    parallels = linspace(nanmin(lat),nanmax(lat),15)\n",
    "    meridians = linspace(nanmin(lon),nanmax(lon),15)\n",
    "    #m.drawparallels(parallels,labels=[1,0,0,1],fontsize=10)\n",
    "    #m.drawmeridians(meridians,labels=[1,0,0,1],fontsize=10)\n",
    "    cb=m.colorbar(res,location=\"right\")\n",
    "    cb.set_label(var_name,fontsize=15)\n",
    "    \n",
    "# function to plot time series\n",
    "def plot_ts(time,SST,line_type,var_name):\n",
    "\n",
    "    # plot\n",
    "    plot_date(time,SST,line_type)\n",
    "    xlabel('Time',fontsize=15)\n",
    "    ylabel(var_name,fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oC-kPabMjJ4Z"
   },
   "source": [
    "**The satellite database:**\n",
    "\n",
    "In this practice, we use 5 years (2011-2015) of satellite observations of the global ocean. We use daily data with 25km spatial resolution. We study the Sea Surface Temperature (SST, in degrees) and the Sea Surface Height (SSH, in meters). SST and SSH are indexed in space (lon, lat) and time. The full database is stored in csv files and is quite large (~50Go)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQWEutFDjJ4a"
   },
   "source": [
    "**Part 1) Manipulation of a small dataset using pandas:**\n",
    "\n",
    "We start to work on a small database (only the 10 first days in 2015, ~250Mo) and we will perform some basic statistics. The idea is to show that even if you use a small sample, basic data manipulation without big data tools is difficult, time and resource consuming.\n",
    "\n",
    "After copying the dataset \"data_sst_ssh_2015_small.csv\" on your Colab environment, we read it using pandas (easy-to-use data structures and data analysis tools). We use the *groupby* and *mean* pandas functions to: map the global SST and SSH (using *plot_im*), plot the mean SST and SSH time series (using *plot_ts*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "ifbClzLQjJ4a",
    "outputId": "52ac4bc4-e72b-4e88-cd06-96fe52bd11b6"
   },
   "outputs": [],
   "source": [
    "# read dataframe\n",
    "data = pd.read_csv('data_sst_ssh_2015_small.csv', header=None, names=['lat', 'lon', 'sst', 'time', 'ssh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "G_D7-HRIjJ4c",
    "outputId": "afc9be0f-2b1d-4a3b-ae5a-fda0f40e61b6"
   },
   "outputs": [],
   "source": [
    "# sample of the dataset\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "rG-YFGfGjJ4e",
    "outputId": "d960f9c5-87f1-4141-cb2f-ed1f86c63662"
   },
   "outputs": [],
   "source": [
    "# statistics of the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "u6mWSi9fjJ4h",
    "outputId": "af4550ef-ae33-4ec0-d8f8-aece511704bf"
   },
   "outputs": [],
   "source": [
    "# plot global SST and SSH maps\n",
    "sst_im=data.groupby(['lon','lat'])['sst'].mean()\n",
    "ssh_im=data.groupby(['lon','lat'])['ssh'].mean()\n",
    "lon_im=data.groupby(['lon','lat'])['lon'].mean()\n",
    "lat_im=data.groupby(['lon','lat'])['lat'].mean()\n",
    "figure()\n",
    "subplot(1,2,1)\n",
    "plot_im(lon_im,lat_im,sst_im,1,'Sea Surface Temperature (째C)')\n",
    "subplot(1,2,2)\n",
    "plot_im(lon_im,lat_im,ssh_im,1,'Sea Surface Height (m)')\n",
    "\n",
    "# plot global SST and SSH time series\n",
    "sst_ts=data.groupby(['time'])['sst'].mean()\n",
    "ssh_ts=data.groupby(['time'])['ssh'].mean()\n",
    "time_ts=data.groupby(['time'])['time'].mean()\n",
    "figure()\n",
    "subplot(2,1,1)\n",
    "plot_ts(time_ts,sst_ts,'-*','Sea Surface Temperature (째C)')\n",
    "subplot(2,1,2)\n",
    "plot_ts(time_ts,ssh_ts,'-*','Sea Surface Height (m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZhJ6Y0ujJ4j"
   },
   "source": [
    "Now, we consider only a small region of the Globe: the Mediterranean Sea. As previously, we plot the mean time series and maps of SST and SSH. We also plot the SSH as a function of SST and the 2 distributions using histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oJp-6L8WjJ4j",
    "outputId": "d577cded-6380-40ee-e351-f1491c34388c"
   },
   "outputs": [],
   "source": [
    "# data selection\n",
    "data_med=data[(data.lon>0) & (data.lon<35) & (data.lat>30) & (data.lat<45)]\n",
    "\n",
    "# plot SST and SSH maps\n",
    "sst_med_im=data_med.groupby(['lon','lat'])['sst'].mean()\n",
    "ssh_med_im=data_med.groupby(['lon','lat'])['ssh'].mean()\n",
    "lon_med_im=data_med.groupby(['lon','lat'])['lon'].mean()\n",
    "lat_med_im=data_med.groupby(['lon','lat'])['lat'].mean()\n",
    "figure()\n",
    "subplot(1,2,1)\n",
    "plot_im(lon_med_im,lat_med_im,sst_med_im,20,'Sea Surface Temperature (째C)')\n",
    "subplot(1,2,2)\n",
    "plot_im(lon_med_im,lat_med_im,ssh_med_im,20,'Sea Surface Height (m)')\n",
    "\n",
    "# plot SST and SSH time series\n",
    "sst_med_ts=data_med.groupby(['time'])['sst'].mean()\n",
    "ssh_med_ts=data_med.groupby(['time'])['ssh'].mean()\n",
    "time_med_ts=data_med.groupby(['time'])['time'].mean()\n",
    "figure()\n",
    "subplot(2,1,1)\n",
    "plot_ts(time_med_ts,sst_med_ts,'-*','Sea Surface Temperature (째C)')\n",
    "subplot(2,1,2)\n",
    "plot_ts(time_med_ts,ssh_med_ts,'-*','Sea Surface Height (m)')\n",
    "\n",
    "# plot hist(SST), hist(SSH), SSH~SST\n",
    "figure()\n",
    "subplot(2,2,1)\n",
    "hist(data_med.sst, 20)\n",
    "xlabel('Sea Surface Temperature (째C)', size=20)\n",
    "subplot(2,2,4)\n",
    "hist(data_med.ssh, 20, orientation=u'horizontal')\n",
    "ylabel('Sea Surface Height (m)', size=20)\n",
    "subplot(2,2,3)\n",
    "scatter(data_med.sst, data_med.ssh)\n",
    "xlabel('Sea Surface Temperature (째C)', size=20)\n",
    "ylabel('Sea Surface Height (m)', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHC6yc4qjJ4m"
   },
   "source": [
    "**Remark:**\n",
    "\n",
    "The dataset used above is small (250Mo) and corresponds only to 10 days. Now, we want to manipulate the full database of 5 years (50Go) using big data tools. This database is stored in the Google Cloud Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBjQ3bgQjJ4m"
   },
   "source": [
    "**Part 2) Manipulation of the full database using BigQuery**\n",
    "\n",
    "First, we have to connect to the Google Cloud Platform. using the following command. You will have to enter the login \"bigdataocean2020@gmail.com\" and password \"bdoimt2020_mce\". Do it only once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "u_YwG5KBjcMe"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oFFgo20y4GR"
   },
   "source": [
    "The Google Cloud Platform has set up a tool to handle large databases. This is called \"BigQuery\" and it is based on the NoSQL (\"Not only SQL\") language. The syntax in BigQuery is very similar to the pandas example given in Part 1). For instance, in the example below, we plot the mean SST map in the Mediterranean Sea over the period 2011-2015. The full 5-years dataset is stored in the BigQuery table *bdo2020.bdo2020.2011_2015*. The project is called *alert-ground-261008* (strange but you have to provide it). The result of the BigQuery request is stored in the *output* dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Gr5GN4BzjJ4n"
   },
   "outputs": [],
   "source": [
    "%%bigquery --project alert-ground-261008 output\n",
    "SELECT lon, lat, AVG(sst) AS mean_sst\n",
    "FROM bdo2020.bdo2020.2011_2015\n",
    "WHERE lon>0 AND lon<35 AND lat>30 AND lat<45\n",
    "GROUP BY lon, lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ZnuQPlOSjJ4p"
   },
   "outputs": [],
   "source": [
    "# sample of the dataframe\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "hC79cj7ajJ4r"
   },
   "outputs": [],
   "source": [
    "plot_im(output.lon,output.lat,output.mean_sst,30,'Sea Surface Temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9WiOePIjJ4u"
   },
   "source": [
    "SQL language is suitable to manipulate and compute basic statistics such as the mean, standard deviation, minimum, maximum, correlation, etc... Here you will find the list of basic functions: https://cloud.google.com/bigquery/docs/reference/legacy-sql?hl=fr.\n",
    "\n",
    "It is also possible to use Machine Learning algorithms inside BigQuery. Here you will find a complete description of the possibilities: https://cloud.google.com/bigquery-ml/docs/bigqueryml-intro?hl=fr. Another solution (easiest way of doing) is to extract a small amount of data in BigQuery and then use scikit-learn: https://scikit-learn.org/stable/.\n",
    "\n",
    "**Important note:** the tables stored in the Google Cloud Platform are not chronologically ordered. When dealing with time series, you will thus have to use *ORDER BY time* in your BigQuery requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IsspP8FjJ4v"
   },
   "source": [
    "**QUESTION 1**: Compute and plot the correlation map over the period 2011-2015 between SST and SSH in the Agulhas current, between longitudes (+15,+70) and latitudes (-50,-30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "SBsJHvJijJ4w"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDKb6EmKjJ42"
   },
   "source": [
    "**QUESTION 2**: Plot the dealy mean and standard deviation time series of SST for the period 2011-2015 in the Equatorial Pacific, between longitudes (+180,+300) and latitudes (-20,+20). The mean SST time series is called the ENSO index and is used to identify El Nino and La Nina events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "MxgxsNIYjJ43"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UWnOjMejJ4_"
   },
   "source": [
    "**QUESTION 3**: Compute the dealy time series of SST over the period 2011-2015 in the Gulf of Mexico, between longitudes (+263,+281) and latitudes (+17,+35). In this region, there are a lot of hurricanes and they use the energy of the sea (hot waters) to get stronger. Thus, after a hurricane, we see a rapid decrease of the SST. Plot the time series of the difference of SST between 2 consecutive days and check that the negative peaks (below -0.4째C) correspond to well known hurricanes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "KFQY8oWbjJ5A"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sx8V4upwjJ5D"
   },
   "source": [
    "**Part 3) Combination of BigQuery and scikit-learn**\n",
    "\n",
    "Now, we will extract data using BigQuery and apply regressions (using scikit-learn) on these extracted data. \n",
    "\n",
    "Note that there is another way to use machine learning algorithms (like the linear regression) in BigQuery, but this solution is difficult to implement: I suggest to use the classic one (i.e., using scikit-learn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eE3_sFiajJ5E"
   },
   "source": [
    "**QUESTION 4**: Model the daily mean SST in the Mediterranean Sea using a linear tendancy and a one-year seasonal cycle such that: $SST(t)=\\alpha_0 + \\alpha_1 t + \\alpha_2 sin(2\\pi\\omega t) + \\alpha_3 cos(2\\pi\\omega t)$, with $\\omega=\\frac{1}{365}$. Then, plot the raw time series and the one produced by the model. Finally, plot the autocorrelation function of the residuals (difference between the raw time series and the estimation made by the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "_7aKEzePjJ5F"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB0MpXf1jJ5K"
   },
   "source": [
    "**QUESTION 5**: Compute the SSH difference between the first day in 2011 and the last day in 2015 for each location in the globe. Then, plot the map of this SSH difference: it shows the sea level rise. Finally, adjust a regression to model the tendancy of the global mean sea level rise (averaged over all the location) and plot the prediction for the horizon 2100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "WHew-wE6jJ5a"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "miniprojet_MCE_SMA_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
